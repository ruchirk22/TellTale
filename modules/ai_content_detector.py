import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import numpy as np
import spacy
import re
from typing import Dict, List, Any
import logging

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class AIContentDetector:
    """
    Detects if text content was likely generated by an AI (e.g., GPT)
    using perplexity, burstiness, and linguistic pattern analysis.
    """

    def __init__(self, device: str = None):
        """
        Initializes the Al detector, loading GPT-2 and spaCy models.
        """
        if device is None:
            self.device = self._get_device()
        else:
            self.device = device
            
        logger.info(f"Initializing AIContentDetector on device: {self.device}")

        try:
            # 1. Load GPT-2 for perplexity
            self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
            self.gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2').to(self.device)
            self.gpt2_model.eval()
            
            # Add padding token if it doesn't exist
            if self.gpt2_tokenizer.pad_token is None:
                self.gpt2_tokenizer.pad_token = self.gpt2_tokenizer.eos_token
                self.gpt2_model.config.pad_token_id = self.gpt2_model.config.eos_token_id
            
            logger.info("GPT-2 model loaded successfully.")

            # 2. Load spaCy for linguistic analysis
            self.nlp = spacy.load('en_core_web_sm')
            logger.info("spaCy 'en_core_web_sm' model loaded successfully.")

        except Exception as e:
            logger.error(f"Error loading models: {e}", exc_info=True)
            raise

    def _get_device(self) -> str:
        """Detects and returns the optimal device (MPS, CUDA, or CPU)."""
        if torch.backends.mps.is_available():
            logger.info("MPS (Metal) device found.")
            return "mps"
        if torch.cuda.is_available():
            logger.info("CUDA device found.")
            return "cuda"
        logger.info("No GPU found, using CPU.")
        return "cpu"

    def analyze_text(self, text: str) -> Dict[str, Any]:
        """
        Performs a comprehensive analysis on the text.

        Args:
            text: The input transcript text.

        Returns:
            A dictionary of AI content features.
        """
        features = {}
        
        # 1. Perplexity Analysis (GPT-2)
        try:
            perplexity_features = self.calculate_perplexity_features(text)
            features.update(perplexity_features)
        except Exception as e:
            logger.error(f"Perplexity calculation failed: {e}", exc_info=True)
            features['perplexity'] = -1
            features['perplexity_burstiness'] = -1

        # 2. Linguistic Pattern Analysis (spaCy)
        try:
            linguistic_features = self.analyze_linguistic_patterns(text)
            features.update(linguistic_features)
        except Exception as e:
            logger.error(f"Linguistic analysis failed: {e}", exc_info=True)

        # 3. AI Content Marker Analysis
        try:
            content_features = self.analyze_content_patterns(text)
            features.update(content_features)
        except Exception as e:
            logger.error(f"Content pattern analysis failed: {e}", exc_info=True)
            
        # 4. Final AI Probability
        features['ai_content_probability'] = self._calculate_ai_probability(features)

        return features

    def calculate_perplexity_features(self, text: str) -> Dict[str, Any]:
        """
        Calculates perplexity and burstiness using GPT-2.
        Lower perplexity = more predictable = more AI-like.
        Lower burstiness = more uniform = more AI-like.
        """
        if not text:
            return {'perplexity': 0, 'perplexity_burstiness': 0, 'low_perplexity_ratio': 0}

        encodings = self.gpt2_tokenizer(
            text, 
            return_tensors='pt', 
            truncation=True, 
            max_length=1024,
            padding=True
        ).to(self.device)
        
        input_ids = encodings.input_ids
        
        with torch.no_grad():
            outputs = self.gpt2_model(input_ids, labels=input_ids)
            loss = outputs.loss
            perplexity = torch.exp(loss).item()

        # Calculate per-token perplexities for burstiness
        # We'll use a simpler proxy for burstiness: std dev of sentence-level PPL
        # as per-token is very slow.
        
        sentence_perplexities = []
        doc = self.nlp(text)
        for sent in doc.sents:
            sent_text = sent.text.strip()
            if not sent_text:
                continue
            
            sent_encodings = self.gpt2_tokenizer(
                sent_text, return_tensors='pt', truncation=True, max_length=1024
            ).to(self.device)
            
            with torch.no_grad():
                sent_outputs = self.gpt2_model(sent_encodings.input_ids, labels=sent_encodings.input_ids)
                sent_ppl = torch.exp(sent_outputs.loss).item()
                sentence_perplexities.append(sent_ppl)

        if not sentence_perplexities:
            return {'perplexity': perplexity, 'perplexity_burstiness': 0, 'low_perplexity_ratio': 0}

        # Burstiness = coefficient of variation of sentence perplexities
        mean_ppl = np.mean(sentence_perplexities)
        std_ppl = np.std(sentence_perplexities)
        burstiness = std_ppl / mean_ppl if mean_ppl > 0 else 0
        
        # Low perplexity ratio (< 30 is a common "AI-like" threshold)
        low_ppl_ratio = np.mean([p < 30 for p in sentence_perplexities])

        return {
            'perplexity': perplexity,
            'perplexity_burstiness': burstiness,
            'low_perplexity_ratio': low_ppl_ratio
        }

    def analyze_linguistic_patterns(self, text: str) -> Dict[str, Any]:
        """Analyzes text with spaCy for linguistic features."""
        doc = self.nlp(text)
        features = {}

        # 1. Lexical Diversity (Type-Token Ratio)
        words = [token.text.lower() for token in doc if token.is_alpha]
        if not words:
            return {}
        
        features['type_token_ratio'] = len(set(words)) / len(words)
        
        # 2. Sentence Length
        sentences = list(doc.sents)
        if sentences:
            sent_lengths = [len(s) for s in sentences]
            features['avg_sentence_length'] = np.mean(sent_lengths)
            features['sentence_length_cv'] = np.std(sent_lengths) / np.mean(sent_lengths)
        
        # 3. Pronoun Usage
        personal_pronouns = len([
            t for t in doc if t.pos_ == 'PRON' and 
            t.text.lower() in ['i', 'me', 'my', 'we', 'us', 'our']
        ])
        features['personal_pronoun_density'] = (personal_pronouns / len(words)) * 100

        return features

    def analyze_content_patterns(self, text: str) -> Dict[str, Any]:
        """Uses regex to find AI-specific markers."""
        features = {}
        text_lower = text.lower()
        
        # 1. Generic phrases (AI loves these)
        generic_phrases = [
            r'it is important to note', r'in conclusion', r'furthermore',
            r'delve into', r'robust solution', r'multifaceted',
            r'leverage', r'synergy', r'paradigm', r'holistic'
        ]
        generic_count = sum(len(re.findall(p, text_lower)) for p in generic_phrases)
        features['generic_phrase_density'] = (generic_count / (len(text.split()) + 1)) * 100

        # 2. Lack of Contractions
        contractions = len(re.findall(r"\w+'\w+", text_lower))
        features['has_contractions'] = int(contractions > 0)

        # 3. Lack of informal words
        informal = len(re.findall(r'\b(yeah|gonna|wanna)\b', text_lower))
        features['has_informal_words'] = int(informal > 0)
        
        return features

    def _calculate_ai_probability(self, features: Dict[str, Any]) -> float:
        """
        Calculates a final AI probability (0-1) based on weighted features.
        This is the heuristic logic from the Master Prompt.
        """
        ai_indicators = 0.0
        total_weights = 0.0
        
        # 1. Perplexity (Low = AI)
        if 'perplexity' in features:
            if features['perplexity'] < 30:
                ai_indicators += (0.3 * (1 - features['perplexity'] / 30)) # Scaled weight
            total_weights += 0.3
            
        # 2. Burstiness (Low = AI)
        if 'perplexity_burstiness' in features:
            if features['perplexity_burstiness'] < 0.5:
                ai_indicators += (0.4 * (1 - features['perplexity_burstiness'] / 0.5))
            total_weights += 0.4
            
        # 3. Generic Phrases (High = AI)
        if 'generic_phrase_density' in features:
            if features['generic_phrase_density'] > 0.02:
                ai_indicators += 0.5
            total_weights += 0.5

        # 4. Personal Pronouns (Low = AI)
        if 'personal_pronoun_density' in features:
            if features['personal_pronoun_density'] < 0.02:
                ai_indicators += 0.4
            total_weights += 0.4
            
        # 5. Perfect Grammar (No contractions/informal = AI)
        if 'has_contractions' in features and not features['has_contractions']:
            ai_indicators += 0.15
        if 'has_informal_words' in features and not features['has_informal_words']:
            ai_indicators += 0.15
        total_weights += 0.3

        if total_weights == 0:
            return 0.5 # Inconclusive

        return min(max(ai_indicators / total_weights, 0.0), 1.0)


if __name__ == "__main__":
    """
    Provides a simple test run for this module.
    No external files are needed.
    
     Instructions:
     1. Activate your virtual environment:
         - macOS / Linux: `source venv/bin/activate`
         - Windows PowerShell: `.\venv\Scripts\Activate.ps1`
         - Windows CMD: `.\venv\Scripts\activate.bat`
     2. Run this module from the 'TellTale/Code/' directory:
         `python -m modules.ai_content_detector`
    """
    logger.info("Running AIContentDetector module test...")
    
    detector = AIContentDetector()

    # 1. Test AI-Generated Text
    logger.info("\n--- Test 1: AI-Generated Text ---")
    ai_text = ("It is important to note that the primary advantage is its "
               "inherent flexibility. Furthermore, the robust architecture "
               "facilitates seamless scalability and simplifies integration.")
    
    ai_features = detector.analyze_text(ai_text)
    logger.info(f"AI Text Features: {ai_features}")
    logger.info(f"AI Text Probability: {ai_features['ai_content_probability']:.2%}")

    # 2. Test Human, Spontaneous Text
    logger.info("\n--- Test 2: Human, Spontaneous Text ---")
    human_text = ("Yeah, so I guess... my main thing was, I really wanted to "
                  "make sure it, y'know, it wouldn't break. I remember one time "
                  "I was working on a similar project and it just, it just fell apart.")
    
    human_features = detector.analyze_text(human_text)
    logger.info(f"Human Text Features: {human_features}")
    logger.info(f"Human Text Probability: {human_features['ai_content_probability']:.2%}")
    
    logger.info("\nAIContentDetector module test complete.")